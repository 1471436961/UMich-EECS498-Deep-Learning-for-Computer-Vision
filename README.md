# UMich-EECS498-Deep-Learning-for-Computer-Vision
The materials I used to study this course in the Winter 2022 semester and my solutions of the assignments.
course webside:https://web.eecs.umich.edu/~justincj/teaching/eecs498/WI2022/
## Lecture slides
### Course Introduction
- Computer vision overview
- Historical context
- Course logistics
### Image Classification
- Data-driven approach
- K-Nearest Neighbor
- Hyperparameters
- Cross-validation
### Linear Classifiers
- Algebraic / Visual / Geometric viewpoints
- Softmax / SVM classifiers
### Regularization + Optimization
- Regularization
- Weight decay
- Stochastic Gradient Descent
- Momentum, AdaGrad, Adam
- Second-order optimizers
### Neural Networks
- Feature transforms
- Fully-connected networks
- Universal approximation
- Convexity
### Backpropagation
- Computational Graphs
- Backpropagation
- Matrix multiplication example
### Convolutional Networks
- Convolution
- Pooling
- Batch Normalization
### CNN Architectures I
- BatchNorm, AlexNet, VGG, ResNet
- Size vs Accuracy
### Training Neural Networks I
- Activation functions
- Data preprocessing
- Weight initialization
### Training Neural Networks II
- Data augmentation
- Regularization (Dropout, etc)
- Learning rate schedules
- Hyperparameter optimization
- Model ensembles
### CNN Architectures II
- Grouped and Separable Convolution
- ResNeXt
- Squeeze-and-Excite
- MobileNets / ShufleNets
- Neural Architecture Search
- EfficientNets
- NFNets
- Revisting ResNets
- RegNets
### Deep Learning Software
- Dynamic vs Static graphs
- PyTorch, TensorFlow
### Object Detection
- Transfer learning
- Object detection task
- R-CNN detector
- Non-Max Suppression (NMS)
- Mean Average Precision (mAP)
### Object Detectors
- Single-stage vs two-stage detectors
- Region Proposal Networks (RPN), Anchor Boxes
- Two-Stage Detectors: Fast R-CNN, Faster R-CNN
- Feature Pyramid Networks
### Image Segmentation
- Single-Stage Detectors: RetinaNet, FCOS
- Semantic segmentation
- Instance segmentation
- Keypoint estimation
### Recurrent Networks
- RNN, LSTM, GRU
- Language modeling
- Sequence-to-sequence
- Image captioning
### Attention
- Multimodal attention
- Self-Attention
- Transformers
### Vision Transformers
- ViT, DeiT
- Swin, MViT
- DETR
- MLP-like architectures
### Generative Models I
- Supervised vs Unsupervised learning
- Discriminative vs Generative models
- Autoregressive models
- Variational Autoencoders
### Generative Models II
- More Variational Autoencoders
- Generative Adversarial Networks
- Normalizing Flows and Diffusion models (maybe)
### Visualizing Models and Generating Images
- Feature visualization
- Adversarial examples
- DeepDream, Style transfer
### Self-Supervised Learning
- Colorization, inpainting
- Contrastive learning
- Masked autoencoding
- Learning from language
### 3D vision
- 3D shape representations
- Depth estimation
- 3D shape prediction
- Voxels, Pointclouds, SDFs, Meshes
- Implicit functions, NeRF
### Videos
- Video classification
- Early / Late fusion
- 3D CNNs
- Two-stream networks
- Transformer-based models
### Conclusion
- Course recap
- The future of computer vision 
## Assignments
### Assignment 1 (state: completed)
- PyTorch warmup
- kNN Classifier
### Assignment 2 (state: completed)
- Linear classifiers
- Two-layer network
### Assignment 3 (state: completed)
- Modular API
- Convolutional Networks
- Batch Normalization
- Autograd
### Assignment 4 (state: wait to complete)
- Object detection
### Assignment 5 (state: completed)
- RNNs
- Image captioning
- Transformers
### Assignment 6 (state: wait to complete)
- Variational Autoencoders
- Generative Adversarial Networks
- Style Transfer
- Feature visualization
